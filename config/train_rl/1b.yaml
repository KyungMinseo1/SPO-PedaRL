train:
  number_of_problems_per_batch: 2
  per_device_train_batch_size: 2
  num_samples_per_problem: 2

  learning_rate: 5e-7
  beta: 0.001
  mu: 2

  reward_list: ["accuracy", "pedagogical_alignment", "think"]
  reward_weights: [0.4, 0.4, 0.2]

  save_policy_to_disk_every_n: 5

  top_k_adv: null
  normalize_tree_advantages: true

teacher_model:
  model_name_or_path: Qwen/Qwen2.5-0.5B-Instruct
  lora:
    enable: true
    rank: 1
    alpha: 1
  vllm:
    temperature: 0.8
    max_length: 8000
    max_num_seqs: 512
    gpu_memory_utilization: 0.5
    number_of_gpus_per_instance: 1
    max_number_of_instances: -1
    load_and_unload: true
    use_v0: true
    enforce_eager: true

student_model:
  model_name_or_path: Qwen/Qwen2.5-0.5B-Instruct
  vllm:
    temperature: 0.6
    max_length: 8000
    max_num_seqs: 512
    gpu_memory_utilization: 0.5
    number_of_gpus_per_instance: 1
    max_number_of_instances: -1
    use_v0: true

    load_and_unload: true

judge_model:
  model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct-AWQ
  vllm:
    temperature: 0.6
    max_length: 8000
    max_num_seqs: 512
    gpu_memory_utilization: 0.6
    number_of_gpus_per_instance: 1
    max_number_of_instances: -1
    load_and_unload: false
    use_awq: true
    use_v0: true

reward_model:
  model_name_or_path: Answer # If we set this to None we get the -r_sol run.

huggingface:
  name: <huggingface_name>
  push_to_hub: false

logging:
  wandb: true
  wandb_project: train-rl_SPO_EXPERIMENT
  wandb_run_name: Qwen2.5-1B-Instruct-(LoRA1)_3R_NormalizedAdv
  run_group: 1b
  wandb_tags: ["1b"]
  save_dir: checkpoints/1b_3R_NormalizedAdv
  save_steps: 5


generation:

  # This turns on thinking.
  # use_thinking: true
  # force_thinking: true

  max_turns: 11
  max_group_size: 3
  branch_size: 3

  # This turns on the hard lambda.
  pedgagogical_advantage_lambda: 0.5
  use_soft_pedagogical_reward: true # If we set this to true we calculate each rewards following the pedagogical rule independently.
  ignore_rejected_judge: true # If we set this to false we get the soft lambda runs. (If you use soft pedagogical reward, set this to true)

  use_experimental_shared_memory: false
  
  # If we turn this on we get the lambda=0.0 run
  number_judge_attempts: 1
  # judges_rules_prompts_paths: {}

dataset:
  max_train_examples: 2000
  lower_bound_solve_rate: 0.2